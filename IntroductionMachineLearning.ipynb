{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a51c01f-353f-4cd9-9738-2014dc22c1dc",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "This is the Jupyter Notebook that contains code for the workshop \"Introduction to Machine Learning\" as part of the SECAI CeTI Summerschool 2023.\n",
    "\n",
    "The notebook should run on different machines, on Binder, Google Colab, locally and on high-performance computers without and with GPU support.\n",
    "\n",
    "If you download only this notebook and want to run it locally, please run the following code cell. It will take several minutes to download and install all required Python packages and download the dataset. If you cloned the entire Github repository, it is recommended to firstly create a virtual environment and install all packages with the scripts provided. You can therefore use the script that is part of the repository. In that case, you do not need to execute the first code cell and instead directly start with the second code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bf2b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/TUD-STKS/SECAI-Summer-School.git\n",
    "%cd SECAI-Summer-School\n",
    "!pip install --upgrade .[notebook]\n",
    "!python -m medmnist download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38932301-52a2-43c9-bb9c-08b55d9e600b",
   "metadata": {},
   "source": [
    "##  Some Python hints\n",
    "\n",
    "Python is a high-level programming language. It consists of a broad variety of Packages that provide functionality to be used for your own projects.\n",
    "\n",
    "The following list gives an overview of some (not all) important Python packages:\n",
    "- [numpy](https://numpy.org/): The fundamental package for scientific computing with Python\n",
    "- [scipy](https://scipy.org/): Fundamental algorithms for scientific computing in Python\n",
    "- [pandas](https://pandas.pydata.org/): Fast, powerful, flexible and easy to use open source data analysis and manipulation tool\n",
    "- [scikit-learn](https://scikit-learn.org/): Machine Learning in Python\n",
    "- [PyTorch](https://pytorch.org/): Machine learning framework based on the Torch library, used for applications originally developed by Meta AI\n",
    "- [tensorflow](https://www.tensorflow.org/): Free and open-source software library for machine learning and artificial intelligence, developed by Google Brain.\n",
    "- [seaborn](https://seaborn.pydata.org/): Statistical data visualization\n",
    "\n",
    "You can import packages in several ways:\n",
    "\n",
    "```python\n",
    "import torch  # Import the entire package\n",
    "import numpy as np  # Import the entire package and use an alias name\n",
    "from sklearn.metrics import accuracy_score  # Import one part of a package\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier  # Import multiple parts of a pacakge\n",
    "```\n",
    "\n",
    "A Python function is a block of code which only runs when it is called. It receives data in form of parameters via arguments. A function can return data as a result.\n",
    "\n",
    "The following code block shows how to define a function with two arguments. The first argument ``a`` is mandatory, i.e., it needs to be passed to the function. The second argument ``b`` is a default parameter. If no value is passed during the function call, the default value is used.\n",
    "\n",
    "The function itself adds ``a`` and ``b`` and returns the result.\n",
    "\n",
    "In the ideal case, every function comes along with a comment that documents what the function does, which parameters and which return values it has.\n",
    "\n",
    "```python\n",
    "def add_constant(a, b=1.1):\n",
    "    \"\"\"\n",
    "    Add a constant to a number.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : number\n",
    "        The number to which a constant is added.\n",
    "    b : number, default = 1.1\n",
    "        The constant to be added with a default value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    c : number\n",
    "        The sum of ``a`` and ``b``\n",
    "    \"\"\"\n",
    "    c = a + b\n",
    "    return c\n",
    "```\n",
    "\n",
    "The function can be called in different ways:\n",
    "- Without specifying the argument that a value is assigned to, the first value is assigned to the first parameter, the second value is assigned to the second parameter, ... .\n",
    "- The result of the function call can be assigned to a a new variable.\n",
    "\n",
    "```python\n",
    "result = add_constant(10)\n",
    "print(result)\n",
    "```\n",
    "\n",
    "Alternatively, the argument that a value is assigned to can be used as a keyword. This is often easier to read.\n",
    "\n",
    "```python\n",
    "result = add_constant(a=10)\n",
    "print(result)\n",
    "```\n",
    "\n",
    "The result of the function call can directly be used as the input to a new function, such as the ``print()`` function.\n",
    "\n",
    "```python\n",
    "print(add_constant(a=10))\n",
    "print(add_constant(a=10, b=1))\n",
    "```\n",
    "\n",
    "Functions are a fundamental concept in Python. Hence, it is worthy to take some time to get familiar functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffacc49-0f71-4b0b-9bc3-0c13a8a3088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import RandomizedSearchCV, PredefinedSplit\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from scipy.stats import loguniform\n",
    "from secai.torch_models import LinearRegression, MultiLayerPerceptron, \\\n",
    "    ConvolutionalNeuralNetwork, LSTMModel, EarlyStopping\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ebfa7d-12f8-49d8-b6f3-e7cc8eaf016c",
   "metadata": {},
   "source": [
    "In this code cell, we define a theme for the visualizations and check, whether we have a GPU available. If so, we use it.\n",
    "\n",
    "Within the workshop, we use Google Colab, which has GPU support. Thus, the device that is printed under the code cell should not be ``device(type='cpu')``. Please ensure that this is the case, otherwise discuss with the presenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6119a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"notebook\")\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14bfd23-76e5-4c08-8446-918da40ac047",
   "metadata": {},
   "source": [
    "## Getting started with a new Machine Learning task\n",
    "\n",
    "We work with the 2D dataset called \"PathMNIST\"\n",
    "\n",
    "The following cell give us some first information about this dataset. We are dealing with an image dataset containing RGB impage patches from hematoxylin & eosin stained histological images, obtained in different clinical centers.\n",
    "\n",
    "Since we are dealing with RGB image patches, we have three different channels.\n",
    "\n",
    "In total, there are nine different classes. Hence, we have a multi-class dataset, and each image is assigned to exactly one class.\n",
    "\n",
    "The training and validation set (NCT-CRC-HE-100K) contain 100,000 patches, and the test set contains 7,180 image patches (CRC-VAL-HE-7K) from a different clinical center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1353f360-8f14-40c3-a39b-af8d232e01ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_INFO = INFO['pathmnist']\n",
    "TASK = DATASET_INFO['task']\n",
    "LABELS = DATASET_INFO['label']\n",
    "\n",
    "N_CHANNELS = DATASET_INFO['n_channels']\n",
    "N_CLASSES = len(DATASET_INFO['label'])\n",
    "\n",
    "DATASET_INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6a398e-f7fd-4aee-80d2-8e0c1bbfd9b9",
   "metadata": {},
   "source": [
    "## Loading the MedMNIST data\n",
    "\n",
    "Since we want to dive deeper into the dataset, we do not apply any kind of preprocessing. We only make sure that the dataset class returns the mean value of the RGB channels, because it is easier to analyze and visualize it. Furthermore, we return the data as `torch.Tensor`, such that we can easily analyze it further.\n",
    "\n",
    "Note that we instantiate three different datasets for training, validation, and for test. This is something that we always need to keep in mind. Always split training and test data and make sure that no test data is used for training or parameter optimization.\n",
    "\n",
    "The constant ``N_PIXELS`` is predefined and simplifies the feature extraction and model definition later. It is the size of the flattened input vector for the linear and for the MLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fce3042-4e40-4f28-b240-d9b7300ffad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PIXELS = 28*28\n",
    "\n",
    "# preprocessing\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# load the data\n",
    "DataClass = getattr(medmnist, DATASET_INFO['python_class'])\n",
    "train_dataset = DataClass(split='train', transform=data_transform, as_rgb=True)\n",
    "validation_dataset = DataClass(split='val', transform=data_transform,\n",
    "                               as_rgb=True)\n",
    "test_dataset = DataClass(split='test', transform=data_transform, as_rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input = []\n",
    "training_target = []\n",
    "for data in tqdm(utils.data.DataLoader(dataset=train_dataset, shuffle=True)):\n",
    "    training_input.append(data[0].numpy().reshape(-1, N_PIXELS))\n",
    "    training_target.append(data[1].numpy().flatten())\n",
    "\n",
    "training_df = pd.DataFrame(np.vstack(training_input),\n",
    "                           columns=[f\"Pixel {k+1}\" for k in range(N_PIXELS)])\n",
    "training_df[\"Target\"] = [LABELS[str(d)] for d in np.hstack(training_target)]\n",
    "training_df[\"Numeric target\"] = np.hstack(training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_input = []\n",
    "validation_target = []\n",
    "for data in tqdm(utils.data.DataLoader(dataset=validation_dataset,\n",
    "                                       shuffle=True)):\n",
    "    validation_input.append(data[0].numpy().reshape(-1, N_PIXELS))\n",
    "    validation_target.append(data[1].numpy().flatten())\n",
    "\n",
    "validation_df = pd.DataFrame(np.vstack(validation_input),\n",
    "                             columns=[f\"Pixel {k+1}\" for k in range(N_PIXELS)])\n",
    "validation_df[\"Target\"] = [\n",
    "    LABELS[str(d)] for d in np.hstack(validation_target)]\n",
    "validation_df[\"Numeric target\"] = np.hstack(validation_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d7b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = []\n",
    "test_target = []\n",
    "for data in tqdm(utils.data.DataLoader(dataset=test_dataset, shuffle=True)):\n",
    "    test_input.append(data[0].numpy().reshape(-1, N_PIXELS))\n",
    "    test_target.append(data[1].numpy().flatten())\n",
    "\n",
    "test_df = pd.DataFrame(np.vstack(test_input),\n",
    "                       columns=[f\"Pixel {k+1}\" for k in range(N_PIXELS)])\n",
    "test_df[\"Target\"] = [LABELS[str(d)] for d in np.hstack(test_target)]\n",
    "test_df[\"Numeric target\"] = np.hstack(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a365c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada42db-806e-49f1-9b8a-9f2f71a2fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec8cf93-1507-4c1f-b795-58d9d1e8b6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ba28e3",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Visualization is always a crucial part when getting started with a new dataset. Even when only looking on samples, we get a better idea of what is contained in the dataset.\n",
    "\n",
    "Here, we observe severa interesting things:\n",
    "\n",
    "- The pixel values (mean of the different RGB values) seem to be normalized, as we do not deal with integer values.\n",
    "- The value ranges are different. In most of the images, the values seem to lie between 0.3 and 0.8, but not always.\n",
    "- The histograms and the boxplots indicate that the pixel values overall are distributed reasonable.\n",
    "- All pixels seem to carry information, as the distribution does not indicate that some pixel values have a small standard deviation.\n",
    "\n",
    "All in all, this suggests that the pre-processing is simple in case of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187d801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, sharex=\"all\", sharey=\"all\")\n",
    "for k in range(9):\n",
    "    sns.heatmap(\n",
    "        data=training_df.loc[\n",
    "            k, [f\"Pixel {k+1}\" for k in range(N_PIXELS)]\n",
    "        ].values.astype(float).reshape(28, 28).T, ax=axs.flatten()[k], \n",
    "        square=True, xticklabels=False, yticklabels=False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4723a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, sharex=\"all\")\n",
    "sns.histplot(data=training_df.loc[:, [f\"Pixel {k+1}\" for k in range(0, 5)]],\n",
    "             ax=axs[0])\n",
    "sns.boxplot(data=training_df.loc[:, [f\"Pixel {k+1}\" for k in range(0, 5)]],\n",
    "            ax=axs[1], orient=\"h\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e1bde4-2b6f-44f7-9e49-d7fb78f6db8b",
   "metadata": {},
   "source": [
    "## Dimensional reduction\n",
    "\n",
    "We will not dive too deep into this topic in the workshop today. Nevertheless, it is important to keep dimensional reduction in the mind. Therefore, we have a quick look Principal Component Analysis (PCA). It mainly rotates the coordinate system such that the new axes (i.e., main components) show in the direction that the very first components explain most of the variance contained in the dataset.\n",
    "\n",
    "The red horizontal line shows how many main components are required to explain at least 95% of the variance contained in the dataset. Hence, it is theoretically sufficient to use approximately 280 main components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2812420",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(training_df.loc[:, [f\"Pixel {k+1}\" for k in range(N_PIXELS)]])\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "sns.lineplot(x=np.arange(1, 785), y=np.cumsum(pca.explained_variance_ratio_),\n",
    "             ax=axs)\n",
    "axs.axhline(y=0.95, c=\"r\")\n",
    "axs.set_xlabel(\"Pixel k\")\n",
    "axs.set_ylabel(\"Accumulated explained variance\")\n",
    "axs.set_xlim((0, N_PIXELS+5))\n",
    "axs.set_ylim((0.55, 1.01))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3568743-3f0d-4615-8e90-83567ed4f02e",
   "metadata": {},
   "source": [
    "## Image classification - from linear classifiers to neural networks\n",
    "\n",
    "It is time to train our first classification model. In general, image classification measn to assign an image to a class. There exists a vast variety of classification methods that we could use. We explore a few of them.\n",
    "\n",
    "Now, let us use a linear classifier as provided by scikit-learn. The ``SGDClassifier`` minimizes the difference between the target and predicted output based on the cross entropy loss function. Before we train the model, we update our preprocessing pipeline. As previously mentioned, we center the data around zero and normalize the data to a range between ``-1`` and ``+1``. Afterwards, we prepare the datasets again as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae1aa27-b117-4795-9a39-e2fd26b87e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "data_transform.transforms.append(transforms.Normalize(mean=[.5], std=[.5]))\n",
    "\n",
    "# load the data\n",
    "DataClass = getattr(medmnist, DATASET_INFO['python_class'])\n",
    "train_dataset = DataClass(split='train', transform=data_transform, as_rgb=True)\n",
    "validation_dataset = DataClass(split='val', transform=data_transform,\n",
    "                               as_rgb=True)\n",
    "test_dataset = DataClass(split='test', transform=data_transform, as_rgb=True)\n",
    "\n",
    "training_input = []\n",
    "training_target = []\n",
    "for data in utils.data.DataLoader(dataset=train_dataset, shuffle=True):\n",
    "    training_input.append(data[0].numpy().reshape(-1, N_PIXELS))\n",
    "    training_target.append(data[1].numpy().flatten())\n",
    "\n",
    "training_df = pd.DataFrame(np.vstack(training_input),\n",
    "                           columns=[f\"Pixel {k+1}\" for k in range(N_PIXELS)])\n",
    "training_df[\"Target\"] = [LABELS[str(d)] for d in np.hstack(training_target)]\n",
    "training_df[\"Numeric target\"] = np.hstack(training_target)\n",
    "\n",
    "validation_input = []\n",
    "validation_target = []\n",
    "for data in utils.data.DataLoader(dataset=validation_dataset, shuffle=True):\n",
    "    validation_input.append(data[0].numpy().reshape(-1, N_PIXELS))\n",
    "    validation_target.append(data[1].numpy().flatten())\n",
    "\n",
    "validation_df = pd.DataFrame(np.vstack(validation_input),\n",
    "                             columns=[f\"Pixel {k+1}\" for k in range(N_PIXELS)])\n",
    "validation_df[\"Target\"] = [\n",
    "    LABELS[str(d)] for d in np.hstack(validation_target)]\n",
    "validation_df[\"Numeric target\"] = np.hstack(validation_target)\n",
    "\n",
    "test_input = []\n",
    "test_target = []\n",
    "for data in utils.data.DataLoader(dataset=test_dataset, shuffle=True):\n",
    "    test_input.append(data[0].numpy().reshape(-1, N_PIXELS))\n",
    "    test_target.append(data[1].numpy().flatten())\n",
    "\n",
    "test_df = pd.DataFrame(np.vstack(test_input),\n",
    "                       columns=[f\"Pixel {k+1}\" for k in range(N_PIXELS)])\n",
    "test_df[\"Target\"] = [LABELS[str(d)] for d in np.hstack(test_target)]\n",
    "test_df[\"Numeric target\"] = np.hstack(test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c463d3b4-4b86-4d7a-88de-3b8997b0317d",
   "metadata": {},
   "source": [
    "Since scikit-learn provides functionality for cross validation with pre-defined splits as it is the case for this dataset, we concatenate the training and validation datasets and mark the training subset with ``-1``, the validation subset with ``0``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e2d63a-2469-45bb-ac38-1641b69da9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_training_df = pd.concat((training_df, validation_df))\n",
    "X_train = cv_training_df.loc[:,\n",
    "          [f\"Pixel {k+1}\" for k in range(N_PIXELS)]].to_numpy()\n",
    "y_train = cv_training_df.loc[:, \"Numeric target\"].to_numpy()\n",
    "test_fold = [-1] * len(training_df) + [1] * len(validation_df)\n",
    "\n",
    "X_test = test_df.loc[:, [f\"Pixel {k+1}\" for k in range(N_PIXELS)]].to_numpy()\n",
    "y_test = test_df.loc[:, \"Numeric target\"].to_numpy()\n",
    "\n",
    "cv = PredefinedSplit(test_fold=test_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e4196e-261c-402a-b77a-0c30de8bb95a",
   "metadata": {},
   "source": [
    "### Train the very first linear regression model\n",
    "\n",
    "scikit-learn makes it very simple to train machine learning models. With only one line of code, we can already train our very first model. One baseline model was already trained in preparation for this workshop, and we load this model. Now, take some time to explore different parameters of the model and observe, how these parameters change the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606f16be-a7d9-445b-a13d-b6fba97963bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    clf = load(\"results/sklearn_linear_model_baseline.joblib\")\n",
    "except FileNotFoundError:\n",
    "    clf = SGDClassifier(loss=\"log_loss\",\n",
    "                        early_stopping=True).fit(X=X_train, y=y_train)\n",
    "    dump(clf, \"results/sklearn_linear_model_baseline.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385be7db-f157-49fe-93b7-8e448c5e00c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518bf2b-c0bf-4bdc-b614-0733f267029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dec5b0-2e60-4798-82c1-0ff7a9db93ae",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization\n",
    "\n",
    "One important aspect is a proper hyper-parameter optimization. Linear models do not have that many hyper-parameters. However, if you have a look in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) of the ``SGDClassifier``, it gets clear that we still have many opportunities to change the basic model. One hyper-parameter that all linear models have in common, is the regularization penalty $\\alpha$, which penalizes large values.\n",
    "\n",
    "scikit-learn offers great model selection tools that allow for a simple hyper-parameter optimization. The code cell below demonstrates this for the regularization penalty ``alpha``. Since this takes a long time, we will not optimize a new model but use a pre-trained model that was prepared for this workshop.\n",
    "\n",
    "The regularization parameter has a significant impact on the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff5679d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    clf = load(\"results/sklearn_linear_model.joblib\")\n",
    "except FileNotFoundError:\n",
    "    clf = RandomizedSearchCV(\n",
    "        estimator=SGDClassifier(loss=\"log_loss\"), n_iter=50, n_jobs=-1, \n",
    "        cv=cv, verbose=10, param_distributions={\n",
    "            \"alpha\": loguniform(a=1e-5, b=1e1)}).fit(X=X_train, y=y_train)\n",
    "    dump(clf, \"results/sklearn_linear_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951abe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "sns.lineplot(data=pd.DataFrame(clf.cv_results_), x=\"param_alpha\",\n",
    "             y=\"mean_test_score\", ax=axs)\n",
    "axs.set_xscale(\"log\")\n",
    "axs.set_xlim((1e-5, 1e1))\n",
    "axs.set_xlabel(\"Ridge parameter alpha\")\n",
    "axs.set_ylabel(\"Validation accuracy\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eeb351",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef92e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c4803c-0bfa-4de1-8961-65b7daf5c248",
   "metadata": {},
   "source": [
    "### From scikit-learn to PyTorch\n",
    "\n",
    "We can also implement linear regression in PyTorch. However, it needs more preparation in terms of code. The advantage is that the same code can be used for almost any model that we implement in PyTorch. Thus, we now step through the different elements that are required to train and evaluate PyTorch models.\n",
    "\n",
    "At first, we define the ``BATCH_SIZE`` as one crucial parameter that should be optimized during a hyper-parameter optimization. You can tune it later when we train deep learning models. It has a significant impact on the performance in terms of both, classification performance and required training times.\n",
    "\n",
    "The next important element is the ``DataLoader`` from PyTorch. This is an important object that handles loading and randomizing the data in batches for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6857a1d-bf1b-4764-bb7c-85171e70afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "PATIENCE = 5\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "train_loader = utils.data.DataLoader(dataset=train_dataset,\n",
    "                                     batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                     num_workers=2, pin_memory=True)\n",
    "validation_loader = utils.data.DataLoader(dataset=validation_dataset,\n",
    "                                          batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                          num_workers=2, pin_memory=True)\n",
    "test_loader = utils.data.DataLoader(dataset=test_dataset,\n",
    "                                    batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                    num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7ff8a8-f99c-4b2b-8f5a-b14987b4bc77",
   "metadata": {},
   "source": [
    "The function to train models looks complicated on a first glance. However, it is actually not that difficult to understand.\n",
    "\n",
    "Essentially, it trains the model at most ``n_epochs``. In every epoch,\n",
    "- the model is trained in minibatches on the entire training dataset,\n",
    "- the model is validated on the entire validation set,\n",
    "- the early stopping criterion is checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504d0134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(torch_model, torch_optimizer, patience, n_epochs, path,\n",
    "                training_dataloader, validation_dataloader):\n",
    "    \"\"\"\n",
    "    Train a PyTorch torch_model with early stopping.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    torch_model : PyTorch torch_model\n",
    "        The torch_model to be optimized.\n",
    "    torch_optimizer : PyTorch optimizer\n",
    "        The optimizer to minimize the bce_loss and update the weights after \n",
    "        each iteration.\n",
    "    patience : int\n",
    "        Number of subsequent training epochs on which the validation accuracy \n",
    "        does not decrease. If this is fulfilled, the training is stopped, and \n",
    "        the best torch_model so far returned.\n",
    "    n_epochs : int\n",
    "        Maximum number of training epochs.\n",
    "    path : Path\n",
    "        Location where to store intermediate models.\n",
    "    training_dataloader : DataLoader\n",
    "        The training input_data loader.\n",
    "    validation_dataloader : DataLoader\n",
    "        The validation input_data loader for early stopping.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch_model : PyTorch torch_model\n",
    "        The optimized torch_model.\n",
    "    optimizer : PyTorch optimizer\n",
    "        The optimizer that is used.\n",
    "    current_epoch : int\n",
    "        The current_epoch at which the training has stopped.\n",
    "    bce_loss : float\n",
    "        The final validation bce_loss.\n",
    "    avg_training_losses : list[float]\n",
    "        The training losses after each current_epoch.\n",
    "    avg_validation_losses : list[float]\n",
    "        The validation losses after each current_epoch.\n",
    "    \"\"\"\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    # to track the training bce_loss as the torch_model trains\n",
    "    training_losses = []\n",
    "    # to track the validation bce_loss as the torch_model trains\n",
    "    validation_losses = []\n",
    "    # to track the average training bce_loss per current_epoch as the\n",
    "    # torch_model trains\n",
    "    avg_training_losses = []\n",
    "    # to track the average validation bce_loss per current_epoch as the\n",
    "    # torch_model trains\n",
    "    avg_validation_losses = []\n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True, path=path)\n",
    "    for current_epoch in range(1, n_epochs + 1):\n",
    "        ###################\n",
    "        # train the torch_model #\n",
    "        ###################\n",
    "        torch_model.train()  # prep torch_model for training\n",
    "        for batch, (input_data, target) in enumerate(training_dataloader, 1):\n",
    "            input_data = input_data.to(DEVICE)\n",
    "            target = target.to(DEVICE)\n",
    "            # clear the gradients of all optimized variables\n",
    "            torch_optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs\n",
    "            output = torch_model(input_data)\n",
    "            # calculate the bce_loss\n",
    "            target = target.squeeze().long()\n",
    "            bce_loss = loss_function(output, target)\n",
    "            # backward pass: gradient of the bce_loss with respect to\n",
    "            # parameters\n",
    "            bce_loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            torch_optimizer.step()\n",
    "            # record training bce_loss\n",
    "            training_losses.append(bce_loss.item())\n",
    "        ######################\n",
    "        # validate the torch_model #\n",
    "        ######################\n",
    "        validation_outputs = []\n",
    "        validation_targets = []\n",
    "        torch_model.eval()  # prep torch_model for evaluation\n",
    "        for input_data, target in validation_dataloader:\n",
    "            input_data = input_data.to(DEVICE)\n",
    "            target = target.to(DEVICE)\n",
    "            # forward pass: compute predicted outputs\n",
    "            output = torch_model(input_data)\n",
    "            # calculate the bce_loss\n",
    "            target = target.squeeze().long()\n",
    "            bce_loss = loss_function(output, target)\n",
    "            # record validation bce_loss\n",
    "            validation_losses.append(bce_loss.item())\n",
    "            validation_targets.append(target.cpu().detach().numpy().flatten())\n",
    "            validation_outputs.append(\n",
    "                output.cpu().detach().numpy().argmax(axis=1))\n",
    "        # print training/validation statistics\n",
    "        # calculate average bce_loss over an current_epoch\n",
    "        training_loss = np.average(training_losses)\n",
    "        validation_loss = np.average(validation_losses)\n",
    "        avg_training_losses.append(training_loss)\n",
    "        avg_validation_losses.append(validation_loss)\n",
    "        epoch_len = len(str(n_epochs))\n",
    "        print_msg = (f'[{current_epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] '\n",
    "                     f'train_loss: {training_loss:.5f} '\n",
    "                     f'valid_loss: {validation_loss:.5f}')\n",
    "        print(print_msg)\n",
    "        # early_stopping needs the validation bce_loss to check if it has\n",
    "        # decresed,\n",
    "        # and if it has, it will make a checkpoint of the current torch_model\n",
    "\n",
    "        early_stopping(-accuracy_score(np.hstack(validation_targets),\n",
    "                                       np.hstack(validation_outputs)),\n",
    "                       torch_model, torch_optimizer, current_epoch)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    return (torch_model, torch_optimizer, early_stopping.epoch, bce_loss,\n",
    "            avg_training_losses, avg_validation_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4059ba3f-d0b9-47f9-b04a-fe6aa8868cae",
   "metadata": {},
   "source": [
    "The function to test models is easier to understand. Essentially, it evaluates the model on the entire training dataset, and on the entire test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cac28e2-f128-4dc5-91b3-74084829ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(torch_model, training_dataloader, test_dataloader):\n",
    "    \"\"\"\n",
    "    Train a PyTorch torch_model with early stopping.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    torch_model : PyTorch torch_model\n",
    "        The torch_model to be optimized.\n",
    "    training_dataloader : DataLoader\n",
    "        The training input_data loader.\n",
    "    test_dataloader : DataLoader\n",
    "        The test input_data loader for early stopping.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    training_targets : list[int]\n",
    "        The training targets.\n",
    "    training_outputs : list[int]\n",
    "        The predictions on the training set.\n",
    "    test_targets : list[int]\n",
    "        The test targets.\n",
    "    test_outputs : list[int]\n",
    "        The predictions on the test set.\n",
    "    \"\"\"\n",
    "    # to track the average accuracy scores on the training dataset\n",
    "    training_outputs = []\n",
    "    training_targets = []\n",
    "    # to track the average accuracy scores on the test dataset\n",
    "    test_outputs = []\n",
    "    test_targets = []\n",
    "    # initialize the early_stopping object\n",
    "    torch_model.eval()  # prep torch_model for inference\n",
    "    with torch.no_grad():\n",
    "        for batch, (input_data, target) in enumerate(training_dataloader, 1):\n",
    "            input_data = input_data.to(DEVICE)\n",
    "            target = target.to(DEVICE)\n",
    "            # forward pass: compute predicted outputs\n",
    "            output = torch_model(input_data)\n",
    "            # Prepare the target output\n",
    "            target = target.squeeze().long()\n",
    "            target = target.float().resize_(len(target), 1)\n",
    "            training_targets.append(target.cpu().detach().numpy().flatten())\n",
    "            training_outputs.append(\n",
    "                output.cpu().detach().numpy().argmax(axis=1))\n",
    "        for batch, (input_data, target) in enumerate(test_dataloader, 1):\n",
    "            input_data = input_data.to(DEVICE)\n",
    "            target = target.to(DEVICE)\n",
    "            # forward pass: compute predicted outputs\n",
    "            output = torch_model(input_data)\n",
    "            # Prepare the target output\n",
    "            target = target.squeeze().long()\n",
    "            target = target.float().resize_(len(target), 1)\n",
    "            test_targets.append(target.cpu().detach().numpy().flatten())\n",
    "            test_outputs.append(output.cpu().detach().numpy().argmax(axis=1))\n",
    "    return training_targets, training_outputs, test_targets, test_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(in_features=N_PIXELS, num_classes=N_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1abd1b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, optimizer, epoch, loss, average_training_losses, \\\n",
    "    average_validation_losses = train_model(model, optimizer, PATIENCE,\n",
    "                                            NUM_EPOCHS,\n",
    "                                            \"results/torch_linear_model.pt\",\n",
    "                                            train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c91cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1),\n",
    "             y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1),\n",
    "             y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, round(len(average_training_losses) / 5) * 5))\n",
    "axs.set_xlabel(\"Number of epochs\")\n",
    "axs.set_ylabel(\"Loss\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cce1e7-131c-466a-81db-0db788a1b42c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"results/torch_linear_model.pt\")\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "y_train_true, y_train_pred, y_test_true, y_test_pred = test_model(\n",
    "    torch_model=model, training_dataloader=train_loader,\n",
    "    test_dataloader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa5d74e-d52f-413b-a838-70ac9b5a51c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.hstack(y_train_true), np.hstack(y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8125e4f0-6b3e-4f4a-8827-5c78ce55f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.hstack(y_test_true), np.hstack(y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d46ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLayerPerceptron(hidden_layer_sizes=(N_PIXELS, ),\n",
    "                             num_classes=N_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02bf0e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, optimizer, epoch, loss, average_training_losses, \\\n",
    "    average_validation_losses = train_model(model, optimizer, PATIENCE,\n",
    "                                            NUM_EPOCHS,\n",
    "                                            \"results/torch_naive_mlp_model.pt\",\n",
    "                                            train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24485eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1),\n",
    "             y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1),\n",
    "             y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, round(len(average_training_losses) / 5) * 5))\n",
    "axs.set_xlabel(\"Number of epochs\")\n",
    "axs.set_ylabel(\"Loss\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21598266-734f-4c75-a3d9-ef1f1cc0a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"results/torch_naive_mlp_model.pt\")\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "y_train_true, y_train_pred, y_test_true, y_test_pred = test_model(\n",
    "    torch_model=model, training_dataloader=train_loader,\n",
    "    test_dataloader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e1f42-72b7-4faa-8abf-5cee634c6069",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.hstack(y_train_true), np.hstack(y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a49a7f4-0a72-472e-8ced-51049f960c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.hstack(y_test_true), np.hstack(y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea08fa16-dde2-4685-a97d-adb8027665cc",
   "metadata": {},
   "source": [
    "## From linear models to the Multilayer Perceptron\n",
    "\n",
    "The Multilayer Perceptron (MLP) is a simple feed-forward neural network, where the input is connected to several hidden layers. After each weight matrix in the hidden layer follows a nonlinear activation function.\n",
    "\n",
    "Compared to linear regression, there are more free parameters to be trained. A huge advantage of these models is that the classes are non-linearly separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d050055",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLayerPerceptron(hidden_layer_sizes=(N_PIXELS, 128, 64, ),\n",
    "                             num_classes=N_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a590b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, optimizer, epoch, loss, average_training_losses, \\\n",
    "    average_validation_losses = train_model(model, optimizer, PATIENCE,\n",
    "                                            NUM_EPOCHS,\n",
    "                                            \"results/torch_deep_mlp_model.pt\",\n",
    "                                            train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1),\n",
    "             y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1),\n",
    "             y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, round(len(average_training_losses) / 5) * 5))\n",
    "axs.set_xlabel(\"Number of epochs\")\n",
    "axs.set_ylabel(\"Loss\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c32148a-29f4-40bc-81f8-8bbb02dd83d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"results/torch_deep_mlp_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "y_train_true, y_train_pred, y_test_true, y_test_pred = test_model(\n",
    "    torch_model=model, training_dataloader=train_loader,\n",
    "    test_dataloader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dbcb69-66c0-463d-8d76-72b1fffb686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.hstack(y_train_true), np.hstack(y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f63a243-b619-4209-88cd-f6245847a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.hstack(y_test_true), np.hstack(y_test_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73708842-b24f-4e44-b36f-b0845061f80b",
   "metadata": {},
   "source": [
    "## From MLPs to Convolutional Neural Networks\n",
    "\n",
    "The Convolutional Neural Network (CNN) is a special kind of feed-forward neural network that consists of several building blocks:\n",
    "- Convolutional layers: Convolve the input to feature maps\n",
    "- Pooling layers: Reduce the dimensionality by combining neighboured elements of the feature maps\n",
    "- Fully connected layer: Traditional MLP that connects the final feature maps to the (classification) outputs\n",
    "\n",
    "In this experiment, we use the following outline:\n",
    "- Convolutional layer with 16 feature maps and a kernel size of 5 and a ReLU nonlinearity\n",
    "- Maximum pooling with a kernel size of 2\n",
    "- Convolutional layer with 32 feature maps and a kernel size of 5 and a ReLU nonlinearity\n",
    "- Maximum pooling with a kernel size of 2\n",
    "- Fully connected layer that maps from the feature maps to the classes, and a ReLU nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ac4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvolutionalNeuralNetwork(in_channels=1, num_classes=N_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe8c07e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, optimizer, epoch, loss, average_training_losses, \\\n",
    "    average_validation_losses = train_model(model, optimizer, PATIENCE,\n",
    "                                            NUM_EPOCHS,\n",
    "                                            \"results/torch_cnn_model.pt\",\n",
    "                                            train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d73d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1),\n",
    "             y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1),\n",
    "             y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, round(len(average_training_losses) / 5) * 5))\n",
    "axs.set_xlabel(\"Number of epochs\")\n",
    "axs.set_ylabel(\"Loss\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd6182-5d82-476f-922c-958157ea804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"results/torch_cnn_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "y_train_true, y_train_pred, y_test_true, y_test_pred = test_model(\n",
    "    torch_model=model, training_dataloader=train_loader,\n",
    "    test_dataloader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df91eb48-f180-4da3-9fe6-87d38f48dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.hstack(y_train_true), np.hstack(y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4248bec-e2f0-4b88-8bba-5e49daa8e522",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.hstack(y_test_true), np.hstack(y_test_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "542896b5-ac2f-44d3-a0e9-83fe3397759e",
   "metadata": {},
   "source": [
    "## From MLPs to Recurrent Neural Networks with Long Short-term Memory\n",
    "\n",
    "The Long Short-Term Memory (LSTM) network is a complex Recurrent Neural Network (RNN) that consists of several building blocks:\n",
    "- Cell: Remembers values over a time\n",
    "- Input gate: decides which new information is stored in the current state\n",
    "- Output gate: controls which part of the current state is output\n",
    "- Forget gate: decides what information to discard from a previous state\n",
    "\n",
    "Take some time to explore different hyper-parameters, such as the parameters that can be seen here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65654e04-32f5-400b-9e0c-a2b00126b6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_size=28, hidden_size=100, num_layers=1,\n",
    "                  bidirectional=False, dropout=0., num_classes=N_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12f5ac1-8814-4d1c-9434-eb22ed231e00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, optimizer, epoch, loss, average_training_losses, \\\n",
    "    average_validation_losses = train_model(model, optimizer, PATIENCE,\n",
    "                                            NUM_EPOCHS,\n",
    "                                            \"results/torch_1L_lstm_model.pt\",\n",
    "                                            train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0937a768-b3e3-4da5-b89b-69dcc52e2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1),\n",
    "             y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1), \n",
    "             y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, round(len(average_training_losses) / 5) * 5))\n",
    "axs.set_xlabel(\"Number of epochs\")\n",
    "axs.set_ylabel(\"Loss\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684c0fdf-d21f-4b94-8cf1-bfa3ce523d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"results/torch_1L_lstm_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "y_train_true, y_train_pred, y_test_true, y_test_pred = test_model(\n",
    "    torch_model=model, training_dataloader=train_loader,\n",
    "    test_dataloader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b469d219-eb23-42e4-a04c-17ddf7d78e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.hstack(y_train_true), np.hstack(y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22d288b-97c2-4f6d-a32c-397eaba68ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.hstack(y_test_true), np.hstack(y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf4db30-b3f3-458b-b6aa-a68f7b79f7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f0db83-73d8-4c96-a48d-f605e55ffdcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a33b42-f9d7-4335-99ff-2dcb9afbb06e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a4056c-3a94-4a52-8b19-f49b21f60bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e572d6-3c24-4ca8-9dcf-9c951cfef1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595509e6-bac4-47c5-93e2-0730f64dc16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_size=28, hidden_size=100, num_layers=2,\n",
    "                  bidirectional=False, dropout=0., num_classes=N_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30f430-a17a-43f6-9598-0d4af91d16a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, optimizer, epoch, loss, average_training_losses, \\\n",
    "    average_validation_losses = train_model(model, optimizer, PATIENCE,\n",
    "                                            NUM_EPOCHS,\n",
    "                                            \"results/torch_2L_lstm_model.pt\",\n",
    "                                            train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2946d1c9-8f85-4964-ac29-ed687e8ffb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1),\n",
    "             y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1),\n",
    "             y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, round(len(average_training_losses) / 5) * 5))\n",
    "axs.set_xlabel(\"Number of epochs\")\n",
    "axs.set_ylabel(\"Loss\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89be64e-cdd6-4116-85df-fa640703a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"results/torch_2L_lstm_model.pt\")\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "y_train_true, y_train_pred, y_test_true, y_test_pred = test_model(\n",
    "    torch_model=model, training_dataloader=train_loader,\n",
    "    test_dataloader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42ec65-d11c-4059-a9c8-c6a021b8bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.hstack(y_train_true), np.hstack(y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e408a-6b84-4dfc-a619-b0b00554dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.hstack(y_test_true), np.hstack(y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1290234-a1a3-4983-95a2-f16ebe07a386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555df117-6dc2-4d7e-af31-6b1b5b7a71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_size=28, hidden_size=100, num_layers=1,\n",
    "                  bidirectional=True, dropout=0., num_classes=N_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aa45f8-4d12-4d66-bdcd-b5559302176c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, optimizer, epoch, loss, average_training_losses,\\\n",
    "    average_validation_losses = train_model(\n",
    "        model, optimizer, PATIENCE, NUM_EPOCHS, \n",
    "        \"results/torch_1L_bi_lstm_model.pt\", train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a904ba0-c100-477d-8b26-d3583c8c8c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1),\n",
    "             y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1), \n",
    "             y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, round(len(average_training_losses) / 5) * 5))\n",
    "axs.set_xlabel(\"Number of epochs\")\n",
    "axs.set_ylabel(\"Loss\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef5e07f-9c37-4256-8193-f9b8a602d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"results/torch_1L_bi_lstm_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "y_train_true, y_train_pred, y_test_true, y_test_pred = test_model(\n",
    "    torch_model=model, training_dataloader=train_loader,\n",
    "    test_dataloader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac451065-b479-425d-a209-dc7f0f6ac044",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.hstack(y_train_true), np.hstack(y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841570b6-b3c2-4f2f-a9b8-dd88cb0fbae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.hstack(y_test_true), np.hstack(y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c009bd57-bcf7-4c7c-b421-a94ca9a0f3e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc18639-6df7-4c67-a629-7651439709e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_size=28, hidden_size=100, num_layers=2,\n",
    "                  bidirectional=True, dropout=0., num_classes=N_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13fae6e-ec68-4c7a-bb61-d8f2eecea7da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, optimizer, epoch, loss, average_training_losses, \\\n",
    "    average_validation_losses = train_model(\n",
    "        model, optimizer, PATIENCE, NUM_EPOCHS,\n",
    "        \"results/torch_2L_bi_lstm_model.pt\", train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb9b72b-f800-4494-a06a-a938d0846308",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1),\n",
    "             y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1),\n",
    "             y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, round(len(average_training_losses) / 5) * 5))\n",
    "axs.set_xlabel(\"Number of epochs\")\n",
    "axs.set_ylabel(\"Loss\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af8d0d-500c-4277-b865-82de77e22e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"results/torch_2L_bi_lstm_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "y_train_true, y_train_pred, y_test_true, y_test_pred = test_model(\n",
    "    torch_model=model, training_dataloader=train_loader,\n",
    "    test_dataloader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740c48d6-0d73-495b-b02d-600f5f52be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.hstack(y_train_true), np.hstack(y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc39d0-b125-4a81-a7dd-65ea31dac565",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.hstack(y_test_true), np.hstack(y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93776723-e4fd-42e1-bd5a-b16b1cccf901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
