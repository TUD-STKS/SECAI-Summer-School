{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bf2b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/TUD-STKS/SECAI-Summer-School.git\n",
    "%cd SECAI-Summer-School\n",
    "!pip install --upgrade .[notebook]\n",
    "!python -m medmnist download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38932301-52a2-43c9-bb9c-08b55d9e600b",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "- Teilnehmende werden fast kein Python kennen\n",
    "- Google Colab\n",
    "- Einführung in Python kurz halten -> evtl. zumindest kurz erklären, was eine Funktion ist\n",
    "- Regression in sklearn\n",
    "- Übergang zu PyTorch -> Motivieren, warum PyTorch für komplexere Modelle besser geignet ist\n",
    "- Englisch!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffacc49-0f71-4b0b-9bc3-0c13a8a3088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedShuffleSplit, PredefinedSplit\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier\n",
    "from scipy.stats import loguniform\n",
    "from secai.torch_models import LinearRegression, MultiLayerPerceptron, ConvolutionalNeuralNetwork, LSTMModel, EarlyStopping\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6119a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"talk\")\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "device = torch.device('cuda') if cuda_available else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14bfd23-76e5-4c08-8446-918da40ac047",
   "metadata": {},
   "source": [
    "# We first work on the 2D dataset called \"PathMNIST\"\n",
    "\n",
    "The following cell give us some first information about this dataset. We are dealing with an image dataset containing RGB impage patches from hematoxylin & eosin stained histological images, obtained in different clinical centers.\n",
    "\n",
    "Since we are dealing with RGB image patches, we have three different channels.\n",
    "\n",
    "In total, there are nine different classes. Hence, we have a multi-class dataset, and each image is assigned to exactly one class.\n",
    "\n",
    "The training and validation set (NCT-CRC-HE-100K) contain 100,000 patches, and the test set contains 7,180 image patches (CRC-VAL-HE-7K) from a different clinical center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1353f360-8f14-40c3-a39b-af8d232e01ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag = 'pathmnist'\n",
    "# data_flag = 'breastmnist'\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "labels = info['label']\n",
    "\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be008a7c",
   "metadata": {},
   "source": [
    "We prepare to download the dataset (if it is not already downloaded) and instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9927e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "download = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6a398e-f7fd-4aee-80d2-8e0c1bbfd9b9",
   "metadata": {},
   "source": [
    "## First, we read the raw MedMNIST data without any preprocessing\n",
    "\n",
    "Since we want to dive deeper into the dataset, we do not apply any kind of preprocessing. We only make sure that the dataset class returns the data as `torch.Tensor`, such that we can easily analyze it further.\n",
    "\n",
    "Note that we create two different datasets, one for training and one for test. This is something that we always need to keep in mind. Always split training and test data and make sure that no test data is used for training or parameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fce3042-4e40-4f28-b240-d9b7300ffad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "N_PIXELS = 28*28\n",
    "\n",
    "validation_split = .2\n",
    "\n",
    "# preprocessing\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5])\n",
    "])\n",
    "\n",
    "# load the data\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "train_dataset = DataClass(split='train', transform=data_transform,\n",
    "                          download=download, as_rgb=True)\n",
    "validation_dataset = DataClass(split='val', transform=data_transform,\n",
    "                               download=download, as_rgb=True)\n",
    "test_dataset = DataClass(split='test', transform=data_transform,\n",
    "                         download=download, as_rgb=True)\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = data.DataLoader(\n",
    "    dataset=validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input = []\n",
    "training_target = []\n",
    "for data in tqdm(train_loader):\n",
    "    training_input.append(data[0].numpy().reshape(-1, N_PIXELS))\n",
    "    training_target.append(data[1].numpy().flatten())\n",
    "\n",
    "training_df = pd.DataFrame(np.vstack(training_input), columns=[f\"Pixel {k+1}\" for k in range(N_PIXELS)])\n",
    "training_df[\"Target\"] = [labels[str(d)] for d in np.hstack(training_target)]\n",
    "training_df[\"Numeric target\"] = np.hstack(training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_input = []\n",
    "validation_target = []\n",
    "for data in tqdm(validation_loader):\n",
    "    validation_input.append(data[0].numpy().reshape(-1, N_PIXELS))\n",
    "    validation_target.append(data[1].numpy().flatten())\n",
    "\n",
    "validation_df = pd.DataFrame(np.vstack(validation_input), columns=[f\"Pixel {k+1}\" for k in range(N_PIXELS)])\n",
    "validation_df[\"Target\"] = [labels[str(d)] for d in np.hstack(validation_target)]\n",
    "validation_df[\"Numeric target\"] = np.hstack(validation_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a365c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada42db-806e-49f1-9b8a-9f2f71a2fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ba28e3",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Visualization is always a crucial part when getting started with a new dataset. Even when only looking on samples, we get a better idea of what is contained in the dataset.\n",
    "\n",
    "Here, we observe severa interesting things:\n",
    "\n",
    "- The pixel values (mean of the different RGB values) seem to be normalized, as we do not deal with integer values.\n",
    "- The value ranges are different. In most of the images, the values seem to lie between 0.3 and 0.8, but not always.\n",
    "- The histograms indicate that the pixel values overall are distributed reasonable.\n",
    "- All pixels seem to carry information, as the distribution does not indicate that some pixel values have a small standard deviation.\n",
    "\n",
    "All in all, this suggests that the pre-processing is simple in case of this dataset. We will simply shift each pixel value by 0.5 and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187d801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, sharex=\"all\", sharey=\"all\")\n",
    "\n",
    "for k in range(9):\n",
    "    sns.heatmap(data=training_df.loc[k, [f\"Pixel {k+1}\" for k in range(N_PIXELS)]].values.astype(float).reshape(28, 28).T,\n",
    "                ax=axs.flatten()[k], square=True, xticklabels=False, yticklabels=False)\n",
    "    # axs.flatten()[k].set_title(training_df.loc[k, \"Target\"])\n",
    "\n",
    "# [ax.set_xlabel(\"x\") for ax in axs[-1, :]]\n",
    "# [ax.set_ylabel(\"y\") for ax in axs[:, 0]]\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4723a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, sharex=\"all\")\n",
    "\n",
    "sns.histplot(data=training_df.loc[:, [f\"Pixel {k+1}\" for k in range(0, 5)]], ax=axs[0])\n",
    "sns.boxplot(data=training_df.loc[:, [f\"Pixel {k+1}\" for k in range(0, 5)]], ax=axs[1], orient=\"h\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"histogram_vs_boxplot.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb01117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(training_df.loc[:, [f\"Pixel {k+1}\" for k in range(N_PIXELS)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2812420",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=np.arange(1, 785), y=np.cumsum(pca.explained_variance_ratio_), ax=axs)\n",
    "axs.axhline(y=0.95, c=\"r\")\n",
    "axs.set_xlabel(\"Pixel k\")\n",
    "axs.set_ylabel(\"Accumulated explained variance\")\n",
    "axs.set_xlim((0, N_PIXELS+5))\n",
    "axs.set_ylim((0.55, 1.01))\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"pca_explained_variance_ratio.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e2d63a-2469-45bb-ac38-1641b69da9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_training_df = pd.concat((training_df, validation_df))\n",
    "test_fold = [-1] * len(training_df) + [1] * len(validation_df)\n",
    "\n",
    "cv = PredefinedSplit(test_fold=test_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff5679d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    clf = load(\"results/sklearn_linear_model.joblib\")\n",
    "except FileNotFoundError:\n",
    "    clf = RandomizedSearchCV(estimator=SGDClassifier(loss=\"log_loss\"), n_iter=50,\n",
    "                             n_jobs=-1, cv=cv, verbose=10,\n",
    "                             param_distributions={\"alpha\": loguniform(a=1e-5, b=1e1)}).fit(\n",
    "        cv_training_df.loc[:, [f\"Pixel {k+1}\" for k in range(N_PIXELS)]], y=cv_training_df.loc[:, \"Numeric target\"])\n",
    "    dump(clf, \"results/sklearn_linear_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951abe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "sns.lineplot(data=pd.DataFrame(clf.cv_results_), x=\"param_alpha\", y=\"mean_test_score\", ax=axs)\n",
    "plt.xscale(\"log\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eeb351",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(training_df.loc[:, [f\"Pixel {k+1}\" for k in range(N_PIXELS)]],\n",
    "          y=training_df.loc[:, \"Numeric target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d7b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = []\n",
    "test_target = []\n",
    "for data in tqdm(test_loader):\n",
    "    test_input.append(data[0].numpy().reshape(-1, N_PIXELS))\n",
    "    test_target.append(data[1].numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becdbe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(np.vstack(test_input), columns=[f\"Pixel {k+1}\" for k in range(N_PIXELS)])\n",
    "test_df[\"Target\"] = [labels[str(d)] for d in np.hstack(test_target)]\n",
    "test_df[\"Numeric target\"] = np.hstack(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef92e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(test_df.loc[:, [f\"Pixel {k+1}\" for k in range(N_PIXELS)]], y=test_df.loc[:, \"Numeric target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c4803c-0bfa-4de1-8961-65b7daf5c248",
   "metadata": {},
   "source": [
    "## Then, we define a simple model for illustration, object function and optimizer that we use to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504d0134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, patience, n_epochs, path, training_dataloader, validation_dataloader):\n",
    "    # to track the training loss as the model trains\n",
    "    training_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    validation_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    average_training_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    average_validation_losses = [] \n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True, path=path)\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        for batch, (data, target) in enumerate(training_dataloader, 1):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            target = target.squeeze().long()\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # record training loss\n",
    "            training_losses.append(loss.item())\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "        for data, target in validation_dataloader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            target = target.squeeze().long()\n",
    "            loss = criterion(output, target)\n",
    "            # record validation loss\n",
    "            validation_losses.append(loss.item())\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        training_loss = np.average(training_losses)\n",
    "        validation_loss = np.average(validation_losses)\n",
    "        average_training_losses.append(training_loss)\n",
    "        average_validation_losses.append(validation_loss)\n",
    "        epoch_len = len(str(n_epochs))\n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] '\n",
    "                     f'train_loss: {training_loss:.5f} '\n",
    "                     f'valid_loss: {validation_loss:.5f}')\n",
    "        print(print_msg)\n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        early_stopping(validation_loss, model, optimizer, epoch)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    return  model, optimizer, early_stopping.epoch, loss, average_training_losses, average_validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(in_features=784, num_classes=n_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1abd1b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patience = 5\n",
    "num_epochs = 200\n",
    "model, optimizer, epoch, loss, average_training_losses, average_validation_losses = train_model(\n",
    "    model, patience, num_epochs, \"results/torch_linear_model.pt\", train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c91cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1), y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1), y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, None))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d46ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLayerPerceptron(hidden_layer_sizes=(784, ), num_classes=n_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02bf0e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patience = 5\n",
    "num_epochs = 200\n",
    "model, optimizer, epoch, loss, average_training_losses, average_validation_losses = train_model(\n",
    "    model, patience, num_epochs, \"results/torch_mlp_model.pt\", train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24485eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1), y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1), y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, None))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d050055",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLayerPerceptron(hidden_layer_sizes=(784, 128, 64, ), num_classes=n_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a590b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 5\n",
    "num_epochs = 200\n",
    "model, optimizer, epoch, loss, average_training_losses, average_validation_losses = train_model(\n",
    "    model, patience, num_epochs, \"results/torch_deep_mlp_model.pt\", train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1), y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1), y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, None))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ac4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvolutionalNeuralNetwork(in_channels=1, num_classes=n_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe8c07e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 5\n",
    "num_epochs = 200\n",
    "model, optimizer, epoch, loss, average_training_losses, average_validation_losses = train_model(\n",
    "    model, patience, num_epochs, \"results/torch_cnn_model.pt\", train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d73d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1), y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1), y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, None))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65654e04-32f5-400b-9e0c-a2b00126b6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_size=28, hidden_size=100, num_layers=1,\n",
    "                 bidirectional=False, dropout=0., num_classes=n_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12f5ac1-8814-4d1c-9434-eb22ed231e00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 5\n",
    "num_epochs = 200\n",
    "model, optimizer, epoch, loss, average_training_losses, average_validation_losses = train_model(\n",
    "    model, patience, num_epochs, \"results/torch_1L_lstm_model.pt\", train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0937a768-b3e3-4da5-b89b-69dcc52e2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1), y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1), y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, None))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595509e6-bac4-47c5-93e2-0730f64dc16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_size=28, hidden_size=100, num_layers=2,\n",
    "                 bidirectional=False, dropout=0., num_classes=n_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30f430-a17a-43f6-9598-0d4af91d16a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 5\n",
    "num_epochs = 200\n",
    "model, optimizer, epoch, loss, average_training_losses, average_validation_losses = train_model(\n",
    "    model, patience, num_epochs, \"results/torch_2L_lstm_model.pt\", train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2946d1c9-8f85-4964-ac29-ed687e8ffb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1), y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1), y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, None))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555df117-6dc2-4d7e-af31-6b1b5b7a71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_size=28, hidden_size=100, num_layers=1,\n",
    "                 bidirectional=True, dropout=0., num_classes=n_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aa45f8-4d12-4d66-bdcd-b5559302176c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 5\n",
    "num_epochs = 200\n",
    "model, optimizer, epoch, loss, average_training_losses, average_validation_losses = train_model(\n",
    "    model, patience, num_epochs, \"results/torch_1L_bi_lstm_model.pt\", train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a904ba0-c100-477d-8b26-d3583c8c8c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1), y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1), y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, None))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc18639-6df7-4c67-a629-7651439709e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_size=28, hidden_size=100, num_layers=2,\n",
    "                 bidirectional=True, dropout=0., num_classes=n_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13fae6e-ec68-4c7a-bb61-d8f2eecea7da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 5\n",
    "num_epochs = 200\n",
    "model, optimizer, epoch, loss, average_training_losses, average_validation_losses = train_model(\n",
    "    model, patience, num_epochs, \"results/torch_2L_bi_lstm_model.pt\", train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb9b72b-f800-4494-a06a-a938d0846308",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1), y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1), y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, None))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af8d0d-500c-4277-b865-82de77e22e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461a6d76-9c6c-45cf-9c46-cc2c8be46c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85459b52-12fe-4a4d-8647-c454e3a78596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d15085c-06f6-4c7a-b9cf-3f34bbd53f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db5031-81d9-4d53-b5a8-aa69d3ca51c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d145106",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"results/torch_lstm_model.pt\")\n",
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14949f80-17bf-4f10-88cc-7ce8dbb07297",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_size=28, hidden_size=100, num_layers=1,\n",
    "                 bidirectional=False, dropout=0., num_classes=n_classes)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['validation_loss']\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87ebe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        if task == 'multi-label, binary-class':\n",
    "            targets = targets.to(torch.float32)\n",
    "        else:\n",
    "            targets = targets.squeeze().long()\n",
    "            targets = targets.float().resize_(len(targets), 1)\n",
    "\n",
    "        y_true.append(targets.detach().numpy().flatten())\n",
    "        y_pred.append(outputs.detach().numpy().argmax(axis=1))\n",
    "accuracy_score(np.hstack(y_true), np.hstack(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d04fbed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        if task == 'multi-label, binary-class':\n",
    "            targets = targets.to(torch.float32)\n",
    "        else:\n",
    "            targets = targets.squeeze().long()\n",
    "            targets = targets.float().resize_(len(targets), 1)\n",
    "\n",
    "        y_true.append(targets.detach().numpy().flatten())\n",
    "        y_pred.append(outputs.detach().numpy().argmax(axis=1))\n",
    "accuracy_score(np.hstack(y_true), np.hstack(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e34a080-c2fd-463f-9e54-1f156656a56b",
   "metadata": {},
   "source": [
    "# We then check a 3D dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf71f61-4d20-4ad2-b7cf-3fbde000aae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag = 'organmnist3d'\n",
    "download = True\n",
    "\n",
    "info = INFO[data_flag]\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "# load the data\n",
    "train_dataset = DataClass(split='train',  download=download)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead571d3-8f9f-4870-a540-b1c5f32cc61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_dataset[0]\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9f579a-01a7-4e41-a0eb-f198f6445e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adca3b0-8650-4858-992e-a76792b9d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = train_dataset.montage(length=1, save_folder=\"tmp/\")\n",
    "frames[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c390b8-a68e-4fd8-8087-af8ba2212036",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = train_dataset.montage(length=20, save_folder=\"tmp/\")\n",
    "\n",
    "frames[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cac28e2-f128-4dc5-91b3-74084829ff0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
