{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bf2b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%git clone https://github.com/TUD-STKS/SECAI-Summer-School.git\n",
    "%cd SECAI-Summer-School\n",
    "%pip install --upgrade .[notebook]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38932301-52a2-43c9-bb9c-08b55d9e600b",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "- Teilnehmende werden fast kein Python kennen\n",
    "- Google Colab\n",
    "- Einführung in Python kurz halten -> evtl. zumindest kurz erklären, was eine Funktion ist\n",
    "- Regression in sklearn\n",
    "- Übergang zu PyTorch -> Motivieren, warum PyTorch für komplexere Modelle besser geignet ist\n",
    "- Englisch!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffacc49-0f71-4b0b-9bc3-0c13a8a3088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedShuffleSplit, PredefinedSplit\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier\n",
    "from scipy.stats import loguniform\n",
    "from secai.torch_models import LinearRegression, MultiLayerPerceptron, ConvolutionalNeuralNetwork, LSTMModel, EarlyStopping\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6119a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"talk\")\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "device = torch.device('cuda') if cuda_available else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14bfd23-76e5-4c08-8446-918da40ac047",
   "metadata": {},
   "source": [
    "# We first work on the 2D dataset called \"PathMNIST\"\n",
    "\n",
    "The following cell give us some first information about this dataset. We are dealing with an image dataset containing RGB impage patches from hematoxylin & eosin stained histological images, obtained in different clinical centers.\n",
    "\n",
    "Since we are dealing with RGB image patches, we have three different channels.\n",
    "\n",
    "In total, there are nine different classes. Hence, we have a multi-class dataset, and each image is assigned to exactly one class.\n",
    "\n",
    "The training and validation set (NCT-CRC-HE-100K) contain 100,000 patches, and the test set contains 7,180 image patches (CRC-VAL-HE-7K) from a different clinical center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1353f360-8f14-40c3-a39b-af8d232e01ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag = 'pathmnist'\n",
    "# data_flag = 'breastmnist'\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "labels = info['label']\n",
    "\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be008a7c",
   "metadata": {},
   "source": [
    "We prepare to download the dataset (if it is not already downloaded) and instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9927e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "download = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6a398e-f7fd-4aee-80d2-8e0c1bbfd9b9",
   "metadata": {},
   "source": [
    "## First, we read the raw MedMNIST data without any preprocessing\n",
    "\n",
    "Since we want to dive deeper into the dataset, we do not apply any kind of preprocessing. We only make sure that the dataset class returns the data as `torch.Tensor`, such that we can easily analyze it further.\n",
    "\n",
    "Note that we create two different datasets, one for training and one for test. This is something that we always need to keep in mind. Always split training and test data and make sure that no test data is used for training or parameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fce3042-4e40-4f28-b240-d9b7300ffad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "N_PIXELS = 28*28\n",
    "\n",
    "validation_split = .2\n",
    "\n",
    "# preprocessing\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5])\n",
    "])\n",
    "\n",
    "# load the data\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "train_dataset = DataClass(split='train', transform=data_transform,\n",
    "                          download=download, as_rgb=True)\n",
    "validation_dataset = DataClass(split='val', transform=data_transform,\n",
    "                               download=download, as_rgb=True)\n",
    "test_dataset = DataClass(split='test', transform=data_transform,\n",
    "                         download=download, as_rgb=True)\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = data.DataLoader(\n",
    "    dataset=validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input = []\n",
    "training_target = []\n",
    "for data in tqdm(train_loader):\n",
    "    training_input.append(data[0].numpy().reshape(-1, N_PIXELS))\n",
    "    training_target.append(data[1].numpy().flatten())\n",
    "\n",
    "training_df = pd.DataFrame(np.vstack(training_input), columns=[f\"Pixel {k+1}\" for k in range(N_PIXELS)])\n",
    "training_df[\"Target\"] = [labels[str(d)] for d in np.hstack(training_target)]\n",
    "training_df[\"Numeric target\"] = np.hstack(training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_input = []\n",
    "validation_target = []\n",
    "for data in tqdm(validation_loader):\n",
    "    validation_input.append(data[0].numpy().reshape(-1, N_PIXELS))\n",
    "    validation_target.append(data[1].numpy().flatten())\n",
    "\n",
    "validation_df = pd.DataFrame(np.vstack(validation_input), columns=[f\"Pixel {k+1}\" for k in range(N_PIXELS)])\n",
    "validation_df[\"Target\"] = [labels[str(d)] for d in np.hstack(validation_target)]\n",
    "validation_df[\"Numeric target\"] = np.hstack(validation_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a365c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada42db-806e-49f1-9b8a-9f2f71a2fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ba28e3",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Visualization is always a crucial part when getting started with a new dataset. Even when only looking on samples, we get a better idea of what is contained in the dataset.\n",
    "\n",
    "Here, we observe severa interesting things:\n",
    "\n",
    "- The pixel values (mean of the different RGB values) seem to be normalized, as we do not deal with integer values.\n",
    "- The value ranges are different. In most of the images, the values seem to lie between 0.3 and 0.8, but not always.\n",
    "- The histograms indicate that the pixel values overall are distributed reasonable.\n",
    "- All pixels seem to carry information, as the distribution does not indicate that some pixel values have a small standard deviation.\n",
    "\n",
    "All in all, this suggests that the pre-processing is simple in case of this dataset. We will simply shift each pixel value by 0.5 and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187d801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, sharex=\"all\", sharey=\"all\")\n",
    "\n",
    "for k in range(9):\n",
    "    sns.heatmap(data=training_df.loc[k, [f\"Pixel {k+1}\" for k in range(N_PIXELS)]].values.astype(float).reshape(28, 28).T,\n",
    "                ax=axs.flatten()[k], square=True, xticklabels=False, yticklabels=False)\n",
    "    # axs.flatten()[k].set_title(training_df.loc[k, \"Target\"])\n",
    "\n",
    "# [ax.set_xlabel(\"x\") for ax in axs[-1, :]]\n",
    "# [ax.set_ylabel(\"y\") for ax in axs[:, 0]]\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4723a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, sharex=\"all\")\n",
    "\n",
    "sns.histplot(data=training_df.loc[:, [f\"Pixel {k+1}\" for k in range(0, 5)]], ax=axs[0])\n",
    "sns.boxplot(data=training_df.loc[:, [f\"Pixel {k+1}\" for k in range(0, 5)]], ax=axs[1], orient=\"h\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"histogram_vs_boxplot.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb01117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(training_df.loc[:, [f\"Pixel {k+1}\" for k in range(N_PIXELS)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2812420",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=np.arange(1, 785), y=np.cumsum(pca.explained_variance_ratio_), ax=axs)\n",
    "axs.axhline(y=0.95, c=\"r\")\n",
    "axs.set_xlabel(\"Pixel k\")\n",
    "axs.set_ylabel(\"Accumulated explained variance\")\n",
    "axs.set_xlim((0, N_PIXELS+5))\n",
    "axs.set_ylim((0.55, 1.01))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pca_explained_variance_ratio.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e2d63a-2469-45bb-ac38-1641b69da9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_training_df = pd.concat((training_df, validation_df))\n",
    "test_fold = [-1] * len(training_df) + [1] * len(validation_df)\n",
    "\n",
    "cv = PredefinedSplit(test_fold=test_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff5679d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    clf = load(\"results/sklearn_linear_model.joblib\")\n",
    "except FileNotFoundError:\n",
    "    clf = RandomizedSearchCV(estimator=SGDClassifier(loss=\"log_loss\"), n_iter=50,\n",
    "                             n_jobs=-1, cv=cv, verbose=10,\n",
    "                             param_distributions={\"alpha\": loguniform(a=1e-5, b=1e1)}).fit(\n",
    "        cv_training_df.loc[:, [f\"Pixel {k+1}\" for k in range(N_PIXELS)]], y=cv_training_df.loc[:, \"Numeric target\"])\n",
    "    dump(clf, \"results/sklearn_linear_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951abe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "sns.lineplot(data=pd.DataFrame(clf.cv_results_), x=\"param_alpha\", y=\"mean_test_score\", ax=axs)\n",
    "plt.xscale(\"log\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eeb351",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(training_df.loc[:, [f\"Pixel {k+1}\" for k in range(N_PIXELS)]],\n",
    "          y=training_df.loc[:, \"Numeric target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d7b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = []\n",
    "test_target = []\n",
    "for data in tqdm(test_loader):\n",
    "    test_input.append(data[0].numpy().reshape(-1, N_PIXELS))\n",
    "    test_target.append(data[1].numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becdbe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(np.vstack(test_input), columns=[f\"Pixel {k+1}\" for k in range(N_PIXELS)])\n",
    "test_df[\"Target\"] = [labels[str(d)] for d in np.hstack(test_target)]\n",
    "test_df[\"Numeric target\"] = np.hstack(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef92e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(test_df.loc[:, [f\"Pixel {k+1}\" for k in range(N_PIXELS)]], y=test_df.loc[:, \"Numeric target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c4803c-0bfa-4de1-8961-65b7daf5c248",
   "metadata": {},
   "source": [
    "## Then, we define a simple model for illustration, object function and optimizer that we use to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504d0134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, patience, n_epochs, path, training_dataloader, validation_dataloader):\n",
    "    # to track the training loss as the model trains\n",
    "    training_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    validation_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    average_training_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    average_validation_losses = [] \n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True, path=path)\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        for batch, (data, target) in enumerate(training_dataloader, 1):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            target = target.squeeze().long()\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # record training loss\n",
    "            training_losses.append(loss.item())\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "        for data, target in validation_dataloader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            target = target.squeeze().long()\n",
    "            loss = criterion(output, target)\n",
    "            # record validation loss\n",
    "            validation_losses.append(loss.item())\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        training_loss = np.average(training_losses)\n",
    "        validation_loss = np.average(validation_losses)\n",
    "        average_training_losses.append(training_loss)\n",
    "        average_validation_losses.append(validation_loss)\n",
    "        epoch_len = len(str(n_epochs))\n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] '\n",
    "                     f'train_loss: {training_loss:.5f} '\n",
    "                     f'valid_loss: {validation_loss:.5f}')\n",
    "        print(print_msg)\n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        early_stopping(validation_loss, model, optimizer, epoch)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    return  model, optimizer, early_stopping.epoch, loss, average_training_losses, average_validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(in_features=784, num_classes=n_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1abd1b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[281/500] train_loss: 1.94192 valid_loss: 1.96417\n",
      "Validation loss decreased (1.964174 --> 1.964172).  Saving model ...\n",
      "[282/500] train_loss: 1.94191 valid_loss: 1.96417\n",
      "Validation loss decreased (1.964172 --> 1.964165).  Saving model ...\n",
      "[283/500] train_loss: 1.94190 valid_loss: 1.96416\n",
      "Validation loss decreased (1.964165 --> 1.964158).  Saving model ...\n",
      "[284/500] train_loss: 1.94189 valid_loss: 1.96416\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[285/500] train_loss: 1.94188 valid_loss: 1.96415\n",
      "Validation loss decreased (1.964158 --> 1.964153).  Saving model ...\n",
      "[286/500] train_loss: 1.94187 valid_loss: 1.96415\n",
      "Validation loss decreased (1.964153 --> 1.964145).  Saving model ...\n",
      "[287/500] train_loss: 1.94186 valid_loss: 1.96414\n",
      "Validation loss decreased (1.964145 --> 1.964140).  Saving model ...\n",
      "[288/500] train_loss: 1.94185 valid_loss: 1.96414\n",
      "Validation loss decreased (1.964140 --> 1.964139).  Saving model ...\n",
      "[289/500] train_loss: 1.94185 valid_loss: 1.96413\n",
      "Validation loss decreased (1.964139 --> 1.964133).  Saving model ...\n",
      "[290/500] train_loss: 1.94184 valid_loss: 1.96413\n",
      "Validation loss decreased (1.964133 --> 1.964130).  Saving model ...\n",
      "[291/500] train_loss: 1.94183 valid_loss: 1.96412\n",
      "Validation loss decreased (1.964130 --> 1.964120).  Saving model ...\n",
      "[292/500] train_loss: 1.94182 valid_loss: 1.96412\n",
      "Validation loss decreased (1.964120 --> 1.964117).  Saving model ...\n",
      "[293/500] train_loss: 1.94181 valid_loss: 1.96411\n",
      "Validation loss decreased (1.964117 --> 1.964112).  Saving model ...\n",
      "[294/500] train_loss: 1.94180 valid_loss: 1.96410\n",
      "Validation loss decreased (1.964112 --> 1.964105).  Saving model ...\n",
      "[295/500] train_loss: 1.94179 valid_loss: 1.96411\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[296/500] train_loss: 1.94179 valid_loss: 1.96410\n",
      "Validation loss decreased (1.964105 --> 1.964102).  Saving model ...\n",
      "[297/500] train_loss: 1.94178 valid_loss: 1.96410\n",
      "Validation loss decreased (1.964102 --> 1.964098).  Saving model ...\n",
      "[298/500] train_loss: 1.94177 valid_loss: 1.96409\n",
      "Validation loss decreased (1.964098 --> 1.964091).  Saving model ...\n",
      "[299/500] train_loss: 1.94176 valid_loss: 1.96408\n",
      "Validation loss decreased (1.964091 --> 1.964082).  Saving model ...\n",
      "[300/500] train_loss: 1.94175 valid_loss: 1.96409\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[301/500] train_loss: 1.94174 valid_loss: 1.96408\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[302/500] train_loss: 1.94174 valid_loss: 1.96408\n",
      "Validation loss decreased (1.964082 --> 1.964081).  Saving model ...\n",
      "[303/500] train_loss: 1.94173 valid_loss: 1.96408\n",
      "Validation loss decreased (1.964081 --> 1.964078).  Saving model ...\n",
      "[304/500] train_loss: 1.94172 valid_loss: 1.96408\n",
      "Validation loss decreased (1.964078 --> 1.964075).  Saving model ...\n",
      "[305/500] train_loss: 1.94171 valid_loss: 1.96407\n",
      "Validation loss decreased (1.964075 --> 1.964068).  Saving model ...\n",
      "[306/500] train_loss: 1.94171 valid_loss: 1.96406\n",
      "Validation loss decreased (1.964068 --> 1.964063).  Saving model ...\n",
      "[307/500] train_loss: 1.94170 valid_loss: 1.96406\n",
      "Validation loss decreased (1.964063 --> 1.964056).  Saving model ...\n",
      "[308/500] train_loss: 1.94169 valid_loss: 1.96405\n",
      "Validation loss decreased (1.964056 --> 1.964054).  Saving model ...\n",
      "[309/500] train_loss: 1.94168 valid_loss: 1.96405\n",
      "Validation loss decreased (1.964054 --> 1.964051).  Saving model ...\n",
      "[310/500] train_loss: 1.94168 valid_loss: 1.96405\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[311/500] train_loss: 1.94167 valid_loss: 1.96405\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[312/500] train_loss: 1.94166 valid_loss: 1.96405\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[313/500] train_loss: 1.94165 valid_loss: 1.96405\n",
      "Validation loss decreased (1.964051 --> 1.964046).  Saving model ...\n",
      "[314/500] train_loss: 1.94164 valid_loss: 1.96404\n",
      "Validation loss decreased (1.964046 --> 1.964036).  Saving model ...\n",
      "[315/500] train_loss: 1.94164 valid_loss: 1.96403\n",
      "Validation loss decreased (1.964036 --> 1.964030).  Saving model ...\n",
      "[316/500] train_loss: 1.94163 valid_loss: 1.96403\n",
      "Validation loss decreased (1.964030 --> 1.964028).  Saving model ...\n",
      "[317/500] train_loss: 1.94162 valid_loss: 1.96402\n",
      "Validation loss decreased (1.964028 --> 1.964018).  Saving model ...\n",
      "[318/500] train_loss: 1.94161 valid_loss: 1.96401\n",
      "Validation loss decreased (1.964018 --> 1.964014).  Saving model ...\n",
      "[319/500] train_loss: 1.94161 valid_loss: 1.96401\n",
      "Validation loss decreased (1.964014 --> 1.964009).  Saving model ...\n",
      "[320/500] train_loss: 1.94160 valid_loss: 1.96401\n",
      "Validation loss decreased (1.964009 --> 1.964005).  Saving model ...\n",
      "[321/500] train_loss: 1.94159 valid_loss: 1.96400\n",
      "Validation loss decreased (1.964005 --> 1.963996).  Saving model ...\n",
      "[322/500] train_loss: 1.94159 valid_loss: 1.96399\n",
      "Validation loss decreased (1.963996 --> 1.963990).  Saving model ...\n",
      "[323/500] train_loss: 1.94158 valid_loss: 1.96399\n",
      "Validation loss decreased (1.963990 --> 1.963986).  Saving model ...\n",
      "[324/500] train_loss: 1.94157 valid_loss: 1.96398\n",
      "Validation loss decreased (1.963986 --> 1.963979).  Saving model ...\n",
      "[325/500] train_loss: 1.94156 valid_loss: 1.96398\n",
      "Validation loss decreased (1.963979 --> 1.963979).  Saving model ...\n",
      "[326/500] train_loss: 1.94156 valid_loss: 1.96397\n",
      "Validation loss decreased (1.963979 --> 1.963973).  Saving model ...\n",
      "[327/500] train_loss: 1.94155 valid_loss: 1.96397\n",
      "Validation loss decreased (1.963973 --> 1.963972).  Saving model ...\n",
      "[328/500] train_loss: 1.94154 valid_loss: 1.96397\n",
      "Validation loss decreased (1.963972 --> 1.963965).  Saving model ...\n",
      "[329/500] train_loss: 1.94154 valid_loss: 1.96396\n",
      "Validation loss decreased (1.963965 --> 1.963957).  Saving model ...\n",
      "[330/500] train_loss: 1.94153 valid_loss: 1.96396\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[331/500] train_loss: 1.94152 valid_loss: 1.96395\n",
      "Validation loss decreased (1.963957 --> 1.963952).  Saving model ...\n",
      "[332/500] train_loss: 1.94152 valid_loss: 1.96395\n",
      "Validation loss decreased (1.963952 --> 1.963952).  Saving model ...\n",
      "[333/500] train_loss: 1.94151 valid_loss: 1.96395\n",
      "Validation loss decreased (1.963952 --> 1.963946).  Saving model ...\n",
      "[334/500] train_loss: 1.94150 valid_loss: 1.96394\n",
      "Validation loss decreased (1.963946 --> 1.963938).  Saving model ...\n",
      "[335/500] train_loss: 1.94150 valid_loss: 1.96394\n",
      "Validation loss decreased (1.963938 --> 1.963936).  Saving model ...\n",
      "[336/500] train_loss: 1.94149 valid_loss: 1.96393\n",
      "Validation loss decreased (1.963936 --> 1.963932).  Saving model ...\n",
      "[337/500] train_loss: 1.94148 valid_loss: 1.96393\n",
      "Validation loss decreased (1.963932 --> 1.963928).  Saving model ...\n",
      "[338/500] train_loss: 1.94148 valid_loss: 1.96392\n",
      "Validation loss decreased (1.963928 --> 1.963922).  Saving model ...\n",
      "[339/500] train_loss: 1.94147 valid_loss: 1.96392\n",
      "Validation loss decreased (1.963922 --> 1.963920).  Saving model ...\n",
      "[340/500] train_loss: 1.94146 valid_loss: 1.96392\n",
      "Validation loss decreased (1.963920 --> 1.963915).  Saving model ...\n",
      "[341/500] train_loss: 1.94146 valid_loss: 1.96391\n",
      "Validation loss decreased (1.963915 --> 1.963910).  Saving model ...\n",
      "[342/500] train_loss: 1.94145 valid_loss: 1.96390\n",
      "Validation loss decreased (1.963910 --> 1.963904).  Saving model ...\n",
      "[343/500] train_loss: 1.94145 valid_loss: 1.96391\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[344/500] train_loss: 1.94144 valid_loss: 1.96390\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[345/500] train_loss: 1.94143 valid_loss: 1.96390\n",
      "Validation loss decreased (1.963904 --> 1.963898).  Saving model ...\n",
      "[346/500] train_loss: 1.94143 valid_loss: 1.96390\n",
      "Validation loss decreased (1.963898 --> 1.963897).  Saving model ...\n",
      "[347/500] train_loss: 1.94142 valid_loss: 1.96389\n",
      "Validation loss decreased (1.963897 --> 1.963891).  Saving model ...\n",
      "[348/500] train_loss: 1.94142 valid_loss: 1.96389\n",
      "Validation loss decreased (1.963891 --> 1.963887).  Saving model ...\n",
      "[349/500] train_loss: 1.94141 valid_loss: 1.96389\n",
      "Validation loss decreased (1.963887 --> 1.963886).  Saving model ...\n",
      "[350/500] train_loss: 1.94140 valid_loss: 1.96388\n",
      "Validation loss decreased (1.963886 --> 1.963879).  Saving model ...\n",
      "[351/500] train_loss: 1.94140 valid_loss: 1.96387\n",
      "Validation loss decreased (1.963879 --> 1.963874).  Saving model ...\n",
      "[352/500] train_loss: 1.94139 valid_loss: 1.96387\n",
      "Validation loss decreased (1.963874 --> 1.963869).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[353/500] train_loss: 1.94139 valid_loss: 1.96386\n",
      "Validation loss decreased (1.963869 --> 1.963863).  Saving model ...\n",
      "[354/500] train_loss: 1.94138 valid_loss: 1.96386\n",
      "Validation loss decreased (1.963863 --> 1.963859).  Saving model ...\n",
      "[355/500] train_loss: 1.94137 valid_loss: 1.96385\n",
      "Validation loss decreased (1.963859 --> 1.963854).  Saving model ...\n",
      "[356/500] train_loss: 1.94137 valid_loss: 1.96385\n",
      "Validation loss decreased (1.963854 --> 1.963848).  Saving model ...\n",
      "[357/500] train_loss: 1.94136 valid_loss: 1.96384\n",
      "Validation loss decreased (1.963848 --> 1.963841).  Saving model ...\n",
      "[358/500] train_loss: 1.94136 valid_loss: 1.96384\n",
      "Validation loss decreased (1.963841 --> 1.963838).  Saving model ...\n",
      "[359/500] train_loss: 1.94135 valid_loss: 1.96383\n",
      "Validation loss decreased (1.963838 --> 1.963832).  Saving model ...\n",
      "[360/500] train_loss: 1.94135 valid_loss: 1.96383\n",
      "Validation loss decreased (1.963832 --> 1.963826).  Saving model ...\n",
      "[361/500] train_loss: 1.94134 valid_loss: 1.96382\n",
      "Validation loss decreased (1.963826 --> 1.963819).  Saving model ...\n",
      "[362/500] train_loss: 1.94134 valid_loss: 1.96382\n",
      "Validation loss decreased (1.963819 --> 1.963816).  Saving model ...\n",
      "[363/500] train_loss: 1.94133 valid_loss: 1.96381\n",
      "Validation loss decreased (1.963816 --> 1.963812).  Saving model ...\n",
      "[364/500] train_loss: 1.94133 valid_loss: 1.96381\n",
      "Validation loss decreased (1.963812 --> 1.963807).  Saving model ...\n",
      "[365/500] train_loss: 1.94132 valid_loss: 1.96381\n",
      "Validation loss decreased (1.963807 --> 1.963806).  Saving model ...\n",
      "[366/500] train_loss: 1.94131 valid_loss: 1.96380\n",
      "Validation loss decreased (1.963806 --> 1.963802).  Saving model ...\n",
      "[367/500] train_loss: 1.94131 valid_loss: 1.96380\n",
      "Validation loss decreased (1.963802 --> 1.963798).  Saving model ...\n",
      "[368/500] train_loss: 1.94130 valid_loss: 1.96379\n",
      "Validation loss decreased (1.963798 --> 1.963791).  Saving model ...\n",
      "[369/500] train_loss: 1.94130 valid_loss: 1.96379\n",
      "Validation loss decreased (1.963791 --> 1.963789).  Saving model ...\n",
      "[370/500] train_loss: 1.94129 valid_loss: 1.96378\n",
      "Validation loss decreased (1.963789 --> 1.963783).  Saving model ...\n",
      "[371/500] train_loss: 1.94129 valid_loss: 1.96378\n",
      "Validation loss decreased (1.963783 --> 1.963779).  Saving model ...\n",
      "[372/500] train_loss: 1.94128 valid_loss: 1.96378\n",
      "Validation loss decreased (1.963779 --> 1.963776).  Saving model ...\n",
      "[373/500] train_loss: 1.94128 valid_loss: 1.96377\n",
      "Validation loss decreased (1.963776 --> 1.963774).  Saving model ...\n",
      "[374/500] train_loss: 1.94127 valid_loss: 1.96377\n",
      "Validation loss decreased (1.963774 --> 1.963771).  Saving model ...\n",
      "[375/500] train_loss: 1.94127 valid_loss: 1.96376\n",
      "Validation loss decreased (1.963771 --> 1.963764).  Saving model ...\n",
      "[376/500] train_loss: 1.94126 valid_loss: 1.96376\n",
      "Validation loss decreased (1.963764 --> 1.963763).  Saving model ...\n",
      "[377/500] train_loss: 1.94126 valid_loss: 1.96376\n",
      "Validation loss decreased (1.963763 --> 1.963760).  Saving model ...\n",
      "[378/500] train_loss: 1.94125 valid_loss: 1.96375\n",
      "Validation loss decreased (1.963760 --> 1.963754).  Saving model ...\n",
      "[379/500] train_loss: 1.94125 valid_loss: 1.96375\n",
      "Validation loss decreased (1.963754 --> 1.963749).  Saving model ...\n",
      "[380/500] train_loss: 1.94124 valid_loss: 1.96375\n",
      "Validation loss decreased (1.963749 --> 1.963745).  Saving model ...\n",
      "[381/500] train_loss: 1.94124 valid_loss: 1.96374\n",
      "Validation loss decreased (1.963745 --> 1.963739).  Saving model ...\n",
      "[382/500] train_loss: 1.94123 valid_loss: 1.96374\n",
      "Validation loss decreased (1.963739 --> 1.963735).  Saving model ...\n",
      "[383/500] train_loss: 1.94123 valid_loss: 1.96373\n",
      "Validation loss decreased (1.963735 --> 1.963731).  Saving model ...\n",
      "[384/500] train_loss: 1.94122 valid_loss: 1.96373\n",
      "Validation loss decreased (1.963731 --> 1.963731).  Saving model ...\n",
      "[385/500] train_loss: 1.94122 valid_loss: 1.96373\n",
      "Validation loss decreased (1.963731 --> 1.963727).  Saving model ...\n",
      "[386/500] train_loss: 1.94121 valid_loss: 1.96372\n",
      "Validation loss decreased (1.963727 --> 1.963722).  Saving model ...\n",
      "[387/500] train_loss: 1.94121 valid_loss: 1.96372\n",
      "Validation loss decreased (1.963722 --> 1.963717).  Saving model ...\n",
      "[388/500] train_loss: 1.94120 valid_loss: 1.96371\n",
      "Validation loss decreased (1.963717 --> 1.963710).  Saving model ...\n",
      "[389/500] train_loss: 1.94120 valid_loss: 1.96370\n",
      "Validation loss decreased (1.963710 --> 1.963704).  Saving model ...\n",
      "[390/500] train_loss: 1.94119 valid_loss: 1.96371\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[391/500] train_loss: 1.94119 valid_loss: 1.96370\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[392/500] train_loss: 1.94118 valid_loss: 1.96370\n",
      "Validation loss decreased (1.963704 --> 1.963702).  Saving model ...\n",
      "[393/500] train_loss: 1.94118 valid_loss: 1.96371\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[394/500] train_loss: 1.94117 valid_loss: 1.96370\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[395/500] train_loss: 1.94117 valid_loss: 1.96371\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[396/500] train_loss: 1.94116 valid_loss: 1.96370\n",
      "Validation loss decreased (1.963702 --> 1.963701).  Saving model ...\n",
      "[397/500] train_loss: 1.94116 valid_loss: 1.96370\n",
      "Validation loss decreased (1.963701 --> 1.963697).  Saving model ...\n",
      "[398/500] train_loss: 1.94115 valid_loss: 1.96370\n",
      "Validation loss decreased (1.963697 --> 1.963696).  Saving model ...\n",
      "[399/500] train_loss: 1.94115 valid_loss: 1.96369\n",
      "Validation loss decreased (1.963696 --> 1.963692).  Saving model ...\n",
      "[400/500] train_loss: 1.94115 valid_loss: 1.96369\n",
      "Validation loss decreased (1.963692 --> 1.963691).  Saving model ...\n",
      "[401/500] train_loss: 1.94114 valid_loss: 1.96369\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[402/500] train_loss: 1.94114 valid_loss: 1.96369\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[403/500] train_loss: 1.94113 valid_loss: 1.96369\n",
      "Validation loss decreased (1.963691 --> 1.963687).  Saving model ...\n",
      "[404/500] train_loss: 1.94113 valid_loss: 1.96368\n",
      "Validation loss decreased (1.963687 --> 1.963684).  Saving model ...\n",
      "[405/500] train_loss: 1.94112 valid_loss: 1.96368\n",
      "Validation loss decreased (1.963684 --> 1.963677).  Saving model ...\n",
      "[406/500] train_loss: 1.94112 valid_loss: 1.96367\n",
      "Validation loss decreased (1.963677 --> 1.963674).  Saving model ...\n",
      "[407/500] train_loss: 1.94112 valid_loss: 1.96367\n",
      "Validation loss decreased (1.963674 --> 1.963673).  Saving model ...\n",
      "[408/500] train_loss: 1.94111 valid_loss: 1.96367\n",
      "Validation loss decreased (1.963673 --> 1.963667).  Saving model ...\n",
      "[409/500] train_loss: 1.94111 valid_loss: 1.96366\n",
      "Validation loss decreased (1.963667 --> 1.963662).  Saving model ...\n",
      "[410/500] train_loss: 1.94110 valid_loss: 1.96366\n",
      "Validation loss decreased (1.963662 --> 1.963659).  Saving model ...\n",
      "[411/500] train_loss: 1.94110 valid_loss: 1.96366\n",
      "Validation loss decreased (1.963659 --> 1.963658).  Saving model ...\n",
      "[412/500] train_loss: 1.94109 valid_loss: 1.96366\n",
      "Validation loss decreased (1.963658 --> 1.963657).  Saving model ...\n",
      "[413/500] train_loss: 1.94109 valid_loss: 1.96366\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[414/500] train_loss: 1.94109 valid_loss: 1.96366\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[415/500] train_loss: 1.94108 valid_loss: 1.96365\n",
      "Validation loss decreased (1.963657 --> 1.963653).  Saving model ...\n",
      "[416/500] train_loss: 1.94108 valid_loss: 1.96365\n",
      "Validation loss decreased (1.963653 --> 1.963651).  Saving model ...\n",
      "[417/500] train_loss: 1.94107 valid_loss: 1.96365\n",
      "Validation loss decreased (1.963651 --> 1.963649).  Saving model ...\n",
      "[418/500] train_loss: 1.94107 valid_loss: 1.96365\n",
      "Validation loss decreased (1.963649 --> 1.963648).  Saving model ...\n",
      "[419/500] train_loss: 1.94107 valid_loss: 1.96364\n",
      "Validation loss decreased (1.963648 --> 1.963643).  Saving model ...\n",
      "[420/500] train_loss: 1.94106 valid_loss: 1.96364\n",
      "Validation loss decreased (1.963643 --> 1.963638).  Saving model ...\n",
      "[421/500] train_loss: 1.94106 valid_loss: 1.96364\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[422/500] train_loss: 1.94105 valid_loss: 1.96363\n",
      "Validation loss decreased (1.963638 --> 1.963633).  Saving model ...\n",
      "[423/500] train_loss: 1.94105 valid_loss: 1.96363\n",
      "Validation loss decreased (1.963633 --> 1.963630).  Saving model ...\n",
      "[424/500] train_loss: 1.94104 valid_loss: 1.96363\n",
      "Validation loss decreased (1.963630 --> 1.963626).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[425/500] train_loss: 1.94104 valid_loss: 1.96362\n",
      "Validation loss decreased (1.963626 --> 1.963623).  Saving model ...\n",
      "[426/500] train_loss: 1.94104 valid_loss: 1.96362\n",
      "Validation loss decreased (1.963623 --> 1.963623).  Saving model ...\n",
      "[427/500] train_loss: 1.94103 valid_loss: 1.96362\n",
      "Validation loss decreased (1.963623 --> 1.963618).  Saving model ...\n",
      "[428/500] train_loss: 1.94103 valid_loss: 1.96362\n",
      "Validation loss decreased (1.963618 --> 1.963616).  Saving model ...\n",
      "[429/500] train_loss: 1.94102 valid_loss: 1.96362\n",
      "Validation loss decreased (1.963616 --> 1.963615).  Saving model ...\n",
      "[430/500] train_loss: 1.94102 valid_loss: 1.96361\n",
      "Validation loss decreased (1.963615 --> 1.963611).  Saving model ...\n",
      "[431/500] train_loss: 1.94102 valid_loss: 1.96361\n",
      "Validation loss decreased (1.963611 --> 1.963609).  Saving model ...\n",
      "[432/500] train_loss: 1.94101 valid_loss: 1.96360\n",
      "Validation loss decreased (1.963609 --> 1.963603).  Saving model ...\n",
      "[433/500] train_loss: 1.94101 valid_loss: 1.96360\n",
      "Validation loss decreased (1.963603 --> 1.963599).  Saving model ...\n",
      "[434/500] train_loss: 1.94101 valid_loss: 1.96360\n",
      "Validation loss decreased (1.963599 --> 1.963595).  Saving model ...\n",
      "[435/500] train_loss: 1.94100 valid_loss: 1.96359\n",
      "Validation loss decreased (1.963595 --> 1.963591).  Saving model ...\n",
      "[436/500] train_loss: 1.94100 valid_loss: 1.96359\n",
      "Validation loss decreased (1.963591 --> 1.963588).  Saving model ...\n",
      "[437/500] train_loss: 1.94099 valid_loss: 1.96358\n",
      "Validation loss decreased (1.963588 --> 1.963583).  Saving model ...\n",
      "[438/500] train_loss: 1.94099 valid_loss: 1.96358\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[439/500] train_loss: 1.94099 valid_loss: 1.96358\n",
      "Validation loss decreased (1.963583 --> 1.963580).  Saving model ...\n",
      "[440/500] train_loss: 1.94098 valid_loss: 1.96357\n",
      "Validation loss decreased (1.963580 --> 1.963575).  Saving model ...\n",
      "[441/500] train_loss: 1.94098 valid_loss: 1.96357\n",
      "Validation loss decreased (1.963575 --> 1.963571).  Saving model ...\n",
      "[442/500] train_loss: 1.94098 valid_loss: 1.96357\n",
      "Validation loss decreased (1.963571 --> 1.963567).  Saving model ...\n",
      "[443/500] train_loss: 1.94097 valid_loss: 1.96356\n",
      "Validation loss decreased (1.963567 --> 1.963565).  Saving model ...\n",
      "[444/500] train_loss: 1.94097 valid_loss: 1.96356\n",
      "Validation loss decreased (1.963565 --> 1.963561).  Saving model ...\n",
      "[445/500] train_loss: 1.94096 valid_loss: 1.96356\n",
      "Validation loss decreased (1.963561 --> 1.963558).  Saving model ...\n",
      "[446/500] train_loss: 1.94096 valid_loss: 1.96356\n",
      "Validation loss decreased (1.963558 --> 1.963556).  Saving model ...\n",
      "[447/500] train_loss: 1.94096 valid_loss: 1.96356\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[448/500] train_loss: 1.94095 valid_loss: 1.96356\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[449/500] train_loss: 1.94095 valid_loss: 1.96355\n",
      "Validation loss decreased (1.963556 --> 1.963552).  Saving model ...\n",
      "[450/500] train_loss: 1.94095 valid_loss: 1.96355\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[451/500] train_loss: 1.94094 valid_loss: 1.96356\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[452/500] train_loss: 1.94094 valid_loss: 1.96355\n",
      "Validation loss decreased (1.963552 --> 1.963551).  Saving model ...\n",
      "[453/500] train_loss: 1.94094 valid_loss: 1.96355\n",
      "Validation loss decreased (1.963551 --> 1.963549).  Saving model ...\n",
      "[454/500] train_loss: 1.94093 valid_loss: 1.96354\n",
      "Validation loss decreased (1.963549 --> 1.963544).  Saving model ...\n",
      "[455/500] train_loss: 1.94093 valid_loss: 1.96354\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[456/500] train_loss: 1.94093 valid_loss: 1.96354\n",
      "Validation loss decreased (1.963544 --> 1.963540).  Saving model ...\n",
      "[457/500] train_loss: 1.94092 valid_loss: 1.96354\n",
      "Validation loss decreased (1.963540 --> 1.963538).  Saving model ...\n",
      "[458/500] train_loss: 1.94092 valid_loss: 1.96354\n",
      "Validation loss decreased (1.963538 --> 1.963536).  Saving model ...\n",
      "[459/500] train_loss: 1.94092 valid_loss: 1.96353\n",
      "Validation loss decreased (1.963536 --> 1.963534).  Saving model ...\n",
      "[460/500] train_loss: 1.94091 valid_loss: 1.96353\n",
      "Validation loss decreased (1.963534 --> 1.963532).  Saving model ...\n",
      "[461/500] train_loss: 1.94091 valid_loss: 1.96353\n",
      "Validation loss decreased (1.963532 --> 1.963529).  Saving model ...\n",
      "[462/500] train_loss: 1.94091 valid_loss: 1.96352\n",
      "Validation loss decreased (1.963529 --> 1.963524).  Saving model ...\n",
      "[463/500] train_loss: 1.94090 valid_loss: 1.96352\n",
      "Validation loss decreased (1.963524 --> 1.963523).  Saving model ...\n",
      "[464/500] train_loss: 1.94090 valid_loss: 1.96352\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[465/500] train_loss: 1.94090 valid_loss: 1.96352\n",
      "Validation loss decreased (1.963523 --> 1.963519).  Saving model ...\n",
      "[466/500] train_loss: 1.94089 valid_loss: 1.96352\n",
      "Validation loss decreased (1.963519 --> 1.963517).  Saving model ...\n",
      "[467/500] train_loss: 1.94089 valid_loss: 1.96351\n",
      "Validation loss decreased (1.963517 --> 1.963513).  Saving model ...\n",
      "[468/500] train_loss: 1.94089 valid_loss: 1.96351\n",
      "EarlyStopping counter: 1 out of 5\n",
      "[469/500] train_loss: 1.94088 valid_loss: 1.96351\n",
      "EarlyStopping counter: 2 out of 5\n",
      "[470/500] train_loss: 1.94088 valid_loss: 1.96351\n",
      "EarlyStopping counter: 3 out of 5\n",
      "[471/500] train_loss: 1.94088 valid_loss: 1.96352\n",
      "EarlyStopping counter: 4 out of 5\n",
      "[472/500] train_loss: 1.94087 valid_loss: 1.96351\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "patience = 5\n",
    "num_epochs = 500\n",
    "model, optimizer, epoch, loss, average_training_losses, average_validation_losses = train_model(\n",
    "    model, patience, num_epochs, \"results/torch_linear_model.pt\", train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c91cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1), y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1), y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, None))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047d46ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLayerPerceptron(hidden_layer_sizes=(784, ), num_classes=n_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02bf0e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patience = 20\n",
    "num_epochs = 500\n",
    "model, optimizer, epoch, loss, average_training_losses, average_validation_losses = train_model(\n",
    "    model, patience, num_epochs, \"results/torch_mlp_model.pt\", train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24485eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1), y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1), y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, None))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d050055",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLayerPerceptron(hidden_layer_sizes=(784, 128, 64, ), num_classes=n_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a590b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 20\n",
    "num_epochs = 500\n",
    "model, optimizer, epoch, loss, average_training_losses, average_validation_losses = train_model(\n",
    "    model, patience, num_epochs, \"results/torch_deep_mlp_model.pt\", train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1), y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1), y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, None))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ac4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvolutionalNeuralNetwork(in_channels=1, num_classes=n_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe8c07e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 20\n",
    "num_epochs = 500\n",
    "model, optimizer, epoch, loss, average_training_losses, average_validation_losses = train_model(\n",
    "    model, patience, num_epochs, \"results/torch_cnn_model.pt\", train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d73d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1), y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1), y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, None))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65654e04-32f5-400b-9e0c-a2b00126b6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_size=28, hidden_size=100, num_layers=1,\n",
    "                 bidirectional=False, dropout=0., num_classes=n_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12f5ac1-8814-4d1c-9434-eb22ed231e00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 20\n",
    "num_epochs = 500\n",
    "model, optimizer, epoch, loss, average_training_losses, average_validation_losses = train_model(\n",
    "    model, patience, num_epochs, \"results/torch_1L_lstm_model.pt\", train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0937a768-b3e3-4da5-b89b-69dcc52e2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1), y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1), y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, None))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595509e6-bac4-47c5-93e2-0730f64dc16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_size=28, hidden_size=100, num_layers=2,\n",
    "                 bidirectional=False, dropout=0., num_classes=n_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30f430-a17a-43f6-9598-0d4af91d16a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 20\n",
    "num_epochs = 500\n",
    "model, optimizer, epoch, loss, average_training_losses, average_validation_losses = train_model(\n",
    "    model, patience, num_epochs, \"results/torch_2L_lstm_model.pt\", train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2946d1c9-8f85-4964-ac29-ed687e8ffb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1), y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1), y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, None))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555df117-6dc2-4d7e-af31-6b1b5b7a71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_size=28, hidden_size=100, num_layers=1,\n",
    "                 bidirectional=True, dropout=0., num_classes=n_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aa45f8-4d12-4d66-bdcd-b5559302176c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 20\n",
    "num_epochs = 500\n",
    "model, optimizer, epoch, loss, average_training_losses, average_validation_losses = train_model(\n",
    "    model, patience, num_epochs, \"results/torch_1L_bi_lstm_model.pt\", train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a904ba0-c100-477d-8b26-d3583c8c8c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1), y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1), y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, None))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc18639-6df7-4c67-a629-7651439709e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_size=28, hidden_size=100, num_layers=2,\n",
    "                 bidirectional=True, dropout=0., num_classes=n_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13fae6e-ec68-4c7a-bb61-d8f2eecea7da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "patience = 20\n",
    "num_epochs = 500\n",
    "model, optimizer, epoch, loss, average_training_losses, average_validation_losses = train_model(\n",
    "    model, patience, num_epochs, \"results/torch_2L_bi_lstm_model.pt\", train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb9b72b-f800-4494-a06a-a938d0846308",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "\n",
    "sns.lineplot(x=np.arange(1, len(average_training_losses) + 1), y=average_training_losses, ax=axs, label=\"Training loss\")\n",
    "sns.lineplot(x=np.arange(1, len(average_validation_losses) + 1), y=average_validation_losses, ax=axs, label=\"Validation loss\")\n",
    "axs.set_xlim((0, None))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af8d0d-500c-4277-b865-82de77e22e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461a6d76-9c6c-45cf-9c46-cc2c8be46c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85459b52-12fe-4a4d-8647-c454e3a78596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d15085c-06f6-4c7a-b9cf-3f34bbd53f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db5031-81d9-4d53-b5a8-aa69d3ca51c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d145106",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"results/torch_lstm_model.pt\")\n",
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14949f80-17bf-4f10-88cc-7ce8dbb07297",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_size=28, hidden_size=100, num_layers=1,\n",
    "                 bidirectional=False, dropout=0., num_classes=n_classes)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['validation_loss']\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87ebe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        if task == 'multi-label, binary-class':\n",
    "            targets = targets.to(torch.float32)\n",
    "        else:\n",
    "            targets = targets.squeeze().long()\n",
    "            targets = targets.float().resize_(len(targets), 1)\n",
    "\n",
    "        y_true.append(targets.detach().numpy().flatten())\n",
    "        y_pred.append(outputs.detach().numpy().argmax(axis=1))\n",
    "accuracy_score(np.hstack(y_true), np.hstack(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d04fbed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        if task == 'multi-label, binary-class':\n",
    "            targets = targets.to(torch.float32)\n",
    "        else:\n",
    "            targets = targets.squeeze().long()\n",
    "            targets = targets.float().resize_(len(targets), 1)\n",
    "\n",
    "        y_true.append(targets.detach().numpy().flatten())\n",
    "        y_pred.append(outputs.detach().numpy().argmax(axis=1))\n",
    "accuracy_score(np.hstack(y_true), np.hstack(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e34a080-c2fd-463f-9e54-1f156656a56b",
   "metadata": {},
   "source": [
    "# We then check a 3D dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf71f61-4d20-4ad2-b7cf-3fbde000aae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag = 'organmnist3d'\n",
    "download = True\n",
    "\n",
    "info = INFO[data_flag]\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "# load the data\n",
    "train_dataset = DataClass(split='train',  download=download)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead571d3-8f9f-4870-a540-b1c5f32cc61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_dataset[0]\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9f579a-01a7-4e41-a0eb-f198f6445e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adca3b0-8650-4858-992e-a76792b9d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = train_dataset.montage(length=1, save_folder=\"tmp/\")\n",
    "frames[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c390b8-a68e-4fd8-8087-af8ba2212036",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = train_dataset.montage(length=20, save_folder=\"tmp/\")\n",
    "\n",
    "frames[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cac28e2-f128-4dc5-91b3-74084829ff0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
